{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P04 Introduction to Keras and CNN\n",
    "\n",
    "In this practical, we will learn how to develop a deep learning model using a popular deep learning tools called Keras. We will implement a model that achieves near state-of-the art performance on the MNIST handwritten digit recognition task.\n",
    "\n",
    "After completing this tutorial, you will be able to do the following:\n",
    "\n",
    "* Learn how to use Keras to implement a baseline neural network model\n",
    "* Learn how to use Keras to implement a simple Convolutional Neural Network (CNN)\n",
    "* Implement a close to state-of-the-art deep learning model for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# <u> The MNIST Handwritten Digit Recognition Problem </u>\n",
    "\n",
    "The MNIST is a dataset for handwritten digit classification that is commonly used to evaluate machine learning models. \n",
    "\n",
    "* The images have been normalized in size and centered. Very little data cleaning or preparation are required, allowing the developer to focus on the machine learning.\n",
    "* It comprises images of 10 digits (0 to 9) taken from a variety of scanned documents. The task is to predict the digits (0 to 9) in each image.\n",
    "* There are 60,000 images for training and 10,000 images for testing. Each image has a size of 28 x 28 pixel (784 pixels total). \n",
    "* Results are reported using classification error (ratio of wrongly classified images). The lower the better.\n",
    "* A good machine learning algorithm is expected to achieve a prediction error of less than 1%. State-of-the-art prediction error of approximately 0.2% can be achieved with large Convolutional Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the MNIST dataset in Keras\n",
    "\n",
    "The Keras deep learning library provides a convenience method (`keras.datasets.mnist.load_data`) for loading the MNIST dataset. The MNIST dataset can be found in the file `~/.keras/datasets/mnist.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# load (download if necessary) the MNIST dataset\n",
    "from keras.datasets import mnist\n",
    "import os\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data(os.sep.join([os.getcwd(),'data', 'mnist.npz']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1: Explore the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get the shape of `X_train`, `X_test`, `y_train`, `y_test` \n",
    "2. Get the type of `X_train` and `y_train`\n",
    "3. Get the value range for `X_train`\n",
    "4. Print out the all possible values of `y_train` and their frequencies\n",
    "5. Display four samples (given for you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (60000, 28, 28)\n",
      "X_test:  (10000, 28, 28)\n",
      "y_train:  (60000,)\n",
      "y_test:  (10000,)\n",
      "X_train range:  0  -  255\n",
      "y_train:  [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_test: \", y_test.shape)\n",
    "print(\"X_train range: \", X_train.min(), \" - \", X_train.max())\n",
    "print(\"y_train: \", np.bincount(y_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we print out the four samples of `X_train` to depict what we are working on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21cc6fb1518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.subplot(221)\n",
    "plt.axis('off')\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.axis('off')\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.axis('off')\n",
    "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.axis('off')\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <u> PART 1: Baseline Model with Multi-Layer Perceptrons (MLP) </u>\n",
    "\n",
    "You can get very good results using a very simple **neural network model** with a **single hidden layer**. \n",
    "\n",
    "In this section we will create a simple multi-layer perceptron model that achieves an error rate of less than 1.74%. We will use this as a baseline for comparing more complex convolutional neural network models. \n",
    "\n",
    "We shall build the following Neural Network:\n",
    "* **input layer**: Every input is flattened to form a 784-D vector\n",
    "* **hidden layer**: 784 units in the hidden layer \n",
    "* **output layer**: 10 possible classes\n",
    "* **loss**: Cross-entropy loss\n",
    "\n",
    "<img src=\"imgs\\P04_MLP1.png\" width=\"50%\">\n",
    "\n",
    "**<u>TODO</u>: Your task for this part is to understand how Keras to build a Neural Network model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Loading and preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let’s start off by importing the classes and functions we will need.\n",
    "\n",
    "%load_ext autoreload\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the random number generator using a fixed seed to ensure that\n",
    "# the results of your script are repeatable.\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data(os.sep.join([os.getcwd(),'data', 'mnist.npz']))\n",
    "\n",
    "num_pixels = X_train.shape[1]*X_train.shape[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2: Data Preprocessing**\n",
    "1. Flatten the 28x28 images (`X_train` and `X_test`) to a 784 vector for each image\n",
    "2. Convert the type from `uint8` to `double`\n",
    "3. Normalize the inputs (`X_train` and `X_test`) from 0-255 to 0-1.\n",
    "4. Encode the output vector (`y_train` and `y_test`) into one-hot embedding representation. For example, if for digits '1', it should encoded as (0, 1, 0, 0, 0, 0, 0, 0, 0, 0). Hints: use `np.utils.to_categorical`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "X_train = X_train.reshape(X_train.shape[0], -1).astype(\"double\")\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).astype(\"double\")\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "X_train -= X_train.min()\n",
    "X_train /= X_train.max()\n",
    "X_test -= X_test.min()\n",
    "X_test /= X_test.max()\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Constructing the MLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to create our simple neural network model. The model is a simple neural network with one hidden layer with the same number of neurons as there are inputs (784). \n",
    "\n",
    "**The Keras Sequential Model**\n",
    "\n",
    "We shall build our model by building a **`Sequential`** model. The `Sequential` model is a linear stack of layers. You can create a `Sequential` model by passing a list of layer instances to the constructor. For example, the following code creates two layers (Dense + Activation -> Dense + Softmax).\n",
    "\n",
    "``` Python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "```\n",
    "\n",
    "* You can also start with an empty `Sequential` model and then add layers via the **`.add()`** method:\n",
    "``` Python\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The first layer**\n",
    "\n",
    "* For the *first layer*, we need to specify the *shape of its input* through the argument `input_shape` or `input_dim` so that the model knows the input shape it should expect. \n",
    "* For the *following layers*, the shapes would be *automatically* inferred and there is no need to specify its shape\n",
    "* Notes on `batch size`: \n",
    "  * In `input_shape`, the batch dimension should not be included.\n",
    "  * If you ever need to specify a fixed batch size for your inputs (this is useful for stateful recurrent networks), you can pass a `batch_size` argument to a layer. If you pass both `batch_size=32` and `input_shape=(6, 8)` to a layer, it will then expect every batch of inputs to have the batch shape  `(32, 6, 8)`.\n",
    "\n",
    "\n",
    "First, we add the hidden layer to our MLP. The hidden layer is a `dense` (fully connected) layer. A layer is added to the `Sequential` model through the `.add()` method. A rectifier activation function is used for the neurons in the hidden layer.\n",
    "\n",
    "* The first parameter specify the number of neurons in the hidden layer. We set the number of neurons in this layer to be the same as the input layer.\n",
    "* Since this is the first layer, we need to specify the input shape (`input_dim=num_pixels`). \n",
    "* Use the ReLU activation layer (`activation='relu'`)\n",
    "* Intialize the kernel (weight values) using  from a truncated normal distribution centered on zero (`kernel_initializer='normal'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The output layer**\n",
    "\n",
    "Next, we add the output layer. \n",
    "* Since this is not the first layer, the input shape of this layer would be computed automatically. \n",
    "* Use the softmax activation on the output layer to turn the output scores into probability-like values and allow one class of the 10 to be selected as the model’s output prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List of core layers in Keras:**\n",
    "\n",
    "In this section, we have used only the `Dense` layer. Keras provides the API for many core layers. They can be found [here](https://keras.io/layers/core/):\n",
    "* Dense\n",
    "* Activation\n",
    "* Dropout\n",
    "* Flatten\n",
    "* Input\n",
    "* Reshape\n",
    "* Permute\n",
    "* Repeat Vector\n",
    "* Lambda\n",
    "* ActivityRegularization\n",
    "* Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Showing the model summary**\n",
    "\n",
    "After you have completed building the model, we can display what we have built through the command **`summary`**. \n",
    "* The first layer should be a dense layer of shape `(None, 784)` where the size of the batch dimension is set to `None`. This means that the batch size is unknown until runtime.\n",
    "* The second layer should be a dense layer of shape `(None, 10)`.\n",
    "\n",
    "The total number of parameters would also be shown (expected value around 623, 290). The number of parameters indicates the size of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 623,290\n",
      "Trainable params: 623,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Configure the learning process (Compile)\n",
    "\n",
    "Before training a model, you need to configure the learning process, which is done via the **`compile`** method. It receives three arguments:\n",
    "\n",
    "* *An optimizer*. This could be the string identifier of an existing optimizer (such as rmsprop or  adagrad), or an instance of the Optimizer class. See: [optimizers](https://keras.io/optimizers/). The optimizers available in Keras include: `sgd`, `adagrad`, `adam`, `adadelta`, `nadam` and `rmsprop`.\n",
    "* *A loss function*. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse), or it can be an objective function. See: [losses](https://keras.io/losses/). The loss functions available in Keras include: `mean_squared_error`, `mean_absoute_error`, `hinge`, `squared_hinge`, `categorical_hinge`, `categorical_crossentropy`, `sparse_categorical_crossentropy`, `binary_crossentropy`, `kullback_leibler_divergence`, etc.\n",
    "\n",
    "* *A list of metrics*. For any classification problem you will want to set this to  metrics=['accuracy']. A metric could be the string identifier of an existing metric or a custom metric function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our model, we shall use the *cross entropy* as our loss function and *ADAM gradient descent* algorithm as our optimizer. Since this is a classification problem, we shall use the *accuracy* as our evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Training\n",
    "\n",
    "We can now fit and evaluate the model. Keras models receives `Numpy arrays` of input data and labels. For training a model, you will typically use the **`fit`** function.\n",
    "\n",
    "**`fit`**: Full documentation on the function is available [here](https://keras.io/models/sequential/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is fit over 10 epochs with updates every 200 images. The test data is used as the validation dataset, allowing you to see the skill of the model as it trains. A verbose value of 2 is used to reduce the output to one line for each training epoch.\n",
    "\n",
    "Remember that we *must never use the testing set to optimize our model*. Here, we have used the testing set as our validation set since our purpose here is simply to show that the model improves over time, and not to fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.2811 - acc: 0.9209 - val_loss: 0.1414 - val_acc: 0.9576\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.1114 - acc: 0.9675 - val_loss: 0.0917 - val_acc: 0.9707\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0713 - acc: 0.9798 - val_loss: 0.0779 - val_acc: 0.9776\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.0502 - acc: 0.9858 - val_loss: 0.0737 - val_acc: 0.9772\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0370 - acc: 0.9894 - val_loss: 0.0673 - val_acc: 0.9796\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0265 - acc: 0.9929 - val_loss: 0.0632 - val_acc: 0.9800\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0207 - acc: 0.9948 - val_loss: 0.0616 - val_acc: 0.9809\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0138 - acc: 0.9970 - val_loss: 0.0619 - val_acc: 0.9804\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.0104 - acc: 0.9980 - val_loss: 0.0584 - val_acc: 0.9813\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0077 - acc: 0.9986 - val_loss: 0.0582 - val_acc: 0.9818\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, the accuracy on the training and validation test is printed each epoch. We can see that both the training and validation accuracy increase over the epochs. On the other hand, the training and validation loss decrease over the iterations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Testing and Evaluation\n",
    "\n",
    "Finally, the test dataset is used to evaluate the model and a classification error rate is printed. The error rate is somewhere around 1.7%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Error: 1.82%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <u>  Part 2 : Simple Convolutional Neural Network for MNIST </u>\n",
    "\n",
    "In the previous section, we have seen how to load the MNIST dataset and train a simple multi-layer perceptron model on it.  In this section, we shall develop a more sophisticated convolutional neural network or CNN model.\n",
    "\n",
    "Keras does provide a lot of capability for creating convolutional neural networks. \n",
    "* The list of *convolutional layers* provided by Keras are listed [here](https://keras.io/layers/convolutional/):\n",
    "  * Conv1D, Conv2D, Separable Conv2D, Conv3D\n",
    "  * Cropping1D, Cropping2D, Cropping3D\n",
    "  * UpSampling1D, UpSampling2D, UpSampling3D\n",
    "  * ZeroPadding1D, ZeroPadding2D, ZeroPadding3D\n",
    "  \n",
    "\n",
    "* The List of *pooling layers* provided by Keras are listed [here](https://keras.io/layers/pooling/)\n",
    "  * MaxPooling1D, MaxPooling2D, MaxPooling3D\n",
    "  * AveragePooling1D, AveragePooling2D, AveragePooling3D\n",
    "  * GlobalMaxPooling1D, GlobalMaxPooling2D\n",
    "  * GlobalAveragePooling1D, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will create a simple CNN for MNIST that demonstrates how to use all of the aspects of a modern CNN implementation, including Convolutional layers, Pooling layers and Dropout layers.\n",
    "\n",
    "We shall create the following CNN model:\n",
    "* **input layer**: Every sample has a size of (?, 32, 32, 1)\n",
    "* **CONV1 layer**: 32 5x5 filters, stride = 1, padding = 0, activation = ReLU. Output size = (?, 24, 24, 32)  \n",
    "* **POOL1 layer**: 2x2 filters, stride = default to pool size, padding = 0. Output size = (?, 12, 12, 32)\n",
    "* **Dropout layer**: Regularization layer. Set the dropout rate to 0.2. Output size = (?, 12, 12, 32)\n",
    "* **Flatten**: Not a real layer. Flatten the previous 2D layer into a 1D vector. Output size = (?, 4608) \n",
    "* **FC1**: Fully connected layer. 128 units. Output size = (?, 128)\n",
    "* **FC2**: Fully connected layer. 10 units. Output size = (?, 10)\n",
    "\n",
    "<img src=\"imgs\\P04_SimpleCNN.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u>TODO</u>: Go through this part and understand how to use the Keras library to build a simple CNN model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "def load_preprocess_MNIST():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data(os.sep.join([os.getcwd(),'data', 'mnist.npz']))\n",
    "\n",
    "    # reshape to be [samples][width][height][pixels]\n",
    "    X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "    X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "    # normalize inputs from 0-255 to 0-1\n",
    "    X_train = X_train / 255\n",
    "    X_test = X_test / 255\n",
    "\n",
    "    # one hot encode outputs\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_preprocess_MNIST()\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Constructing the Simple CNN \n",
    "\n",
    "** Creating a Sequential Model**\n",
    "\n",
    "First, we create an empty Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding CONV1**\n",
    "\n",
    "First, we add the **CONV1** layer:\n",
    "\n",
    "* `filters`: The first parameter is the number of filters.\n",
    "* `kernel_size`: The second parameter is the size of each filter.\n",
    "* `input_shape = (28, 28, 1)`: Since this is the first layer, the shape of the input must be specified. Here, we do not specify the batch volume to allow arbitrary batch sizes during training. \n",
    "* `strides = 1`: This argument is not really necessary. Default value for strides is 1. \n",
    "* `padding = valid` : This argument is not really necessary. Default value for padding is `valid` which means that there is no padding. Without padding, the output volume has  a smaller receptive field. To maintain the receptive field, set padding to `same`. \n",
    "* `data_format = 'channels_last'` indicates that the position of the channel dimension. Some tools like Tensorflow expects the channel at the last dimension (28, 28, 1) (Set to `'channels_last'`) whereas some tools like Theano expects the channel at the first dimension (1, 28, 28) (Set to `'channels_first'`).\n",
    "* `activation = 'relu'`: use ReLU activations which has been proven to work very well for deep models. \n",
    "\n",
    "Since the input receptive field N = 28, the filter size F = 5 and stride S = 1, the output receptive field = (N - F)/S + 1 = (28 - 5)/1 + 1 = 24. Since we use 32 filters, the output layer of CONV1 has a size of (24, 24, 32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, \n",
    "                 (5, 5), \n",
    "                 input_shape=(28, 28, 1), \n",
    "                 strides = 1,     \n",
    "                 padding = 'valid', \n",
    "                 data_format = 'channels_last', \n",
    "                 activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding POOL1**\n",
    "\n",
    "Next, we add a max pooling layer:\n",
    "* `pool_size = (2, 2)`: the first parameter is the size of the filter.\n",
    "* `strides = 1`: This argument is not really necessary. Default value for strides is 1. \n",
    "* `padding = valid` : This argument is not really necessary. Default value for padding is `valid` which means that there is no padding. Without padding, the output volume has  a smaller receptive field. To maintain the receptive field, set padding to `same`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add( MaxPooling2D(pool_size=(2, 2),\n",
    "                       strides = 2, \n",
    "                       padding = 'valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding DROPOUT1**\n",
    "\n",
    "Then, we apply dropout to the output of POOL1. This helps to prevent overfitting. We set the dropout rate (ratio of units to be be drop) to 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flattening and Adding FC1**\n",
    "\n",
    "First, we flatten the 2D input (shape = (12, 12, 32)) to a 1D vector (shape = (4608)). Then, we add a fully connected layer with 128 units with `ReLU` activation. Therefore, the output is a vector of size 128 per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding the output layer FC2**\n",
    "\n",
    "Lastly, we insert the output layer. \n",
    "* Since there are 10 classes, the output layer has 10 units. \n",
    "* Use the softmax activation on the output layer to turn the output scores into probability-like values and allow one class of the 10 to be selected as the model’s output prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done constructing our model. Let's show what we have built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 592,074\n",
      "Trainable params: 592,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Configure the learning process (Compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Training the Model\n",
    "\n",
    "The CNN is fit over 10 epochs with a batch size of 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2222 - acc: 0.9370 - val_loss: 0.0763 - val_acc: 0.9774\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.0710 - acc: 0.9791 - val_loss: 0.0527 - val_acc: 0.9832\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.0507 - acc: 0.9844 - val_loss: 0.0413 - val_acc: 0.9854\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.0385 - acc: 0.9881 - val_loss: 0.0393 - val_acc: 0.9870\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.0315 - acc: 0.9899 - val_loss: 0.0343 - val_acc: 0.9873\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.0260 - acc: 0.9921 - val_loss: 0.0374 - val_acc: 0.9884\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.0217 - acc: 0.9934 - val_loss: 0.0353 - val_acc: 0.9889\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.0191 - acc: 0.9937 - val_loss: 0.0311 - val_acc: 0.9890\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.0153 - acc: 0.9951 - val_loss: 0.0280 - val_acc: 0.9905\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.0133 - acc: 0.9959 - val_loss: 0.0328 - val_acc: 0.9902\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Evaluating the Model\n",
    "\n",
    "Finally, the test dataset is used to evaluate the model and a classification error rate is printed. The error rate is somewhere around 1.03%. This is better than our simple multi-layer perceptron model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Error: 0.98%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <u>  Part 3 : Larger Convolutional Neural Network for MNIST </u>\n",
    "\n",
    "Now that we have seen how to create a simple CNN, let’s take a look at a model capable of close to state of the art results.\n",
    "\n",
    "In this section, we will create a simple CNN for MNIST that demonstrates how to use all of the aspects of a modern CNN implementation, including Convolutional layers, Pooling layers and Dropout layers.\n",
    "\n",
    "We shall create the following CNN model:\n",
    "* **input layer**: Every input images has a shape of (28, 28, 1)\n",
    "* **CONV1 layer**: 30 5x5 filters, stride = 1, padding = 0, activation = ReLU  \n",
    "* **POOL1 layer**: 2x2 filters, stride = default to pool size, padding = 0. \n",
    "* **CONV2 layer**: 30 3x3 filters, stride = 1, padding = 0, activation = ReLU  \n",
    "* **POOL2 layer**: 2x2 filters, stride = default to pool size, padding = 0. \n",
    "* **Dropout layer**: Regularization layer. Set the dropout rate to 0.2.\n",
    "* **Flatten**: Not a real layer. Flatten the previous 2D layer into a 1D vector.\n",
    "* **FC1**: Fully connected layer. 128 units, ReLU. \n",
    "* **FC2**: Fully connected layer. 50 units, ReLU. \n",
    "* **FC3**: Fully connected layer. 10 units, Softmax.\n",
    "\n",
    "<img src=\"imgs\\P04_LargerCNN.png\" width=\"50%\">\n",
    "\n",
    "**Exercise #3: Identify the size of the output volume for each layer**\n",
    "* **input layer**: <u>(?, 28, 28, 1)</u>\n",
    "* **CONV1 layer**: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "* **POOL1 layer**: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "* **CONV2 layer**: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "* **POOL2 layer**: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "* **Dropout layer**: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "* **Flatten**: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "* **FC1**: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "* **FC2**: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "* **FC3**: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to load  first step is to import the classes and functions needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = load_preprocess_MNIST()\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise #4: Construct the larger CNN described above.**  \n",
    "\n",
    "In the following, we shall build the larger CNN model described above. Complete the function `larger_model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def larger_model():\n",
    "    ############################################################\n",
    "    # Create the larger CNN model described above\n",
    "    ############################################################\n",
    "    model = None\n",
    "    # Your code here\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(30, (5, 5), \n",
    "                 input_shape=(28, 28, 1), \n",
    "                 strides = 1,     \n",
    "                 padding = 'valid', \n",
    "                 data_format = 'channels_last', \n",
    "                 activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),\n",
    "                       strides = 2, \n",
    "                       padding = 'valid'))\n",
    "    model.add(Conv2D(15, (3, 3), \n",
    "                 strides = 1,     \n",
    "                 padding = 'valid', \n",
    "                 data_format = 'channels_last', \n",
    "                 activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),\n",
    "                   strides = 2, \n",
    "                   padding = 'valid'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    ############################################################\n",
    "    # Add the learning modules (use the crossentropy loss \n",
    "    # function, adam optimizer and accuracy metric)\n",
    "    ############################################################\n",
    "    # Your code here\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = larger_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 30)        780       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 30)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 10, 10, 15)        4065      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 15)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 5, 5, 15)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 375)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               48128     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 50)                6450      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 59,933\n",
      "Trainable params: 59,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise #5: Train the model**\n",
    "\n",
    "Train the model using 10 epochs and a batch size of 200. Validate your data with the test set. Remember not to perform any model optimization using the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.3972 - acc: 0.8750 - val_loss: 0.0921 - val_acc: 0.9712\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.0977 - acc: 0.9708 - val_loss: 0.0541 - val_acc: 0.9828\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.0702 - acc: 0.9784 - val_loss: 0.0485 - val_acc: 0.9844\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.0565 - acc: 0.9823 - val_loss: 0.0317 - val_acc: 0.9893\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.0469 - acc: 0.9854 - val_loss: 0.0325 - val_acc: 0.9892\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.0411 - acc: 0.9868 - val_loss: 0.0289 - val_acc: 0.9900\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.0394 - acc: 0.9877 - val_loss: 0.0310 - val_acc: 0.9903\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.0340 - acc: 0.9893 - val_loss: 0.0352 - val_acc: 0.9885\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.0311 - acc: 0.9900 - val_loss: 0.0275 - val_acc: 0.9916\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.0283 - acc: 0.9909 - val_loss: 0.0232 - val_acc: 0.9914\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise #6: Evaluate your model**\n",
    "\n",
    "Evaluate your model using the test set. You can expect an error rate of around 0.8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Error: 0.86%\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
