{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P03 Implementing a Neural Network\n",
    "\n",
    "In this practical, we are going to develop the code for a two layer neural network to perform classification.\n",
    "\n",
    "* Implement a **two layer neural network** \n",
    "* Implement the **softmax loss function** to train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lib.neural_net import TwoLayerNet\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Description\n",
    "\n",
    "Today, we are going to build the following  neural network:\n",
    "\n",
    "* **input layer**: Every sample has a total of four features \n",
    "* **hidden layer**: 10 nodes per samplem\n",
    "* **output layer**: 3 possible classes\n",
    "* **loss**: Softmax loss\n",
    "\n",
    "<img src=\"imgs\\P03_twolayernet.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset\n",
    "\n",
    "To start, let's create some toy data to check on your  implementations. Note that we set the random seed to 0 for repeatable experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 5 # five samples\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1) \n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the neural net code\n",
    "\n",
    "In this section, we shall implement the two layer neural net in the class **`TwoLayerNet`** in **`lib/neural_net.py`**. \n",
    "\n",
    "\n",
    " The network parameters are stored in the instance variable **`self.params`** where keys are string parameter names and values are numpy arrays (Refer to the function **`__init__`**). \n",
    "\n",
    "The following code initializes the neural net. Again, we set the random seed to a fixed value for repeatable experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) \n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1: Implement the forward pass (compute scores)**\n",
    "\n",
    "Open the file **`lib/neural_net.py`** and look at the method **`TwoLayerNet.loss`**. This function is very similar to the loss functions you have written for the SVM exercises: It takes the data and weights and computes the class scores, the loss, and the gradients on the parameters. \n",
    "\n",
    "Implement the first part of the forward pass which uses the weights and biases to compute the scores for all inputs. Note that the scores is a matrix of size `[NxC]` where `N` is the number of samples and `C` is the number of classes.\n",
    "\n",
    "If you have implemented the section correctly, then the difference between your score and the correct score should be very small (i.e., < 1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = net.loss(X)\n",
    "print('Your scores:\\n', scores)\n",
    "print('\\ncorrect scores:')\n",
    "correct_scores = np.asarray([\n",
    "  [-0.81233741, -1.27654624, -0.70335995],\n",
    "  [-0.17129677, -1.18803311, -0.47310444],\n",
    "  [-0.51590475, -1.01354314, -0.8504215 ],\n",
    "  [-0.15419291, -0.48629638, -0.52901952],\n",
    "  [-0.00618733, -0.12435261, -0.15226949]])\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "diff = np.sum(np.abs(scores - correct_scores))\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(diff)\n",
    "if diff < 1e-7:\n",
    "    print('\\nCongrats! You have implemented the forward function correctly')\n",
    "else:\n",
    "    print('\\nSorry, you still need to work on your code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2: Implement the softmax loss function**\n",
    "\n",
    "In the same function, implement the second part that computes the data and regularizaion loss.\n",
    "\n",
    "If your implementation is correct, the difference between your loss and the correct loss should be very small (i.e., < 1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, _ = net.loss(X, y, reg=0.05)\n",
    "correct_loss = 1.30378789133\n",
    "\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(np.sum(np.abs(loss - correct_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3: Implement the predict function**\n",
    "\n",
    "Now implement the predict function to predict the label of a dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10) \n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "y_pred = net.predict(X)\n",
    "\n",
    "print(\"Predicted:\", y_pred)\n",
    "print(\"Expected: [0, 1, 1, 1, 2]\")\n",
    "\n",
    "if np.sum(y_pred == [0, 1, 1, 1, 2]) == num_inputs:\n",
    "    print(\"Congratulations! You have implemented the predict function correctly\")\n",
    "else:\n",
    "    print(\"Opps! Some problem with your implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "To train the network we will use Stochastic gradient descent (SGD), similar to our multi-class SVM linear classifier. \n",
    "\n",
    "\n",
    "**Exercise 4: Stochastic gradient descent**\n",
    "\n",
    "Complete the second section function **`TwoLayerNet.train`** and fill in the missing sections to implement the training procedure. This should be very similar to the training procedure you used in the previous practical. \n",
    "\n",
    "Once you have implemented the method, run the code below to train a two-layer network on toy data. You should achieve a training loss less than 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0) \n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "stats_no_schedule = net.train(\n",
    "            X, y, \n",
    "            X, y,\n",
    "            learning_rate=1e-1, \n",
    "            learning_rate_decay=1.0,\n",
    "            reg=5e-6,\n",
    "            num_epochs=40, \n",
    "            verbose=1)\n",
    "\n",
    "print('Final training loss: ', stats_no_schedule['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats_no_schedule['loss_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5: Implement a learning rate schedule**\n",
    "\n",
    "When training deep neural networks, it is often useful to reduce learning rate as the training progresses. This can be done by using pre-defined learning rate schedules or adaptive learning rate methods. In this practical, we shall adjust the learning rate at every epoch where we decay `learning_rate` at a rate of `learning_rate_decay`.\n",
    "\n",
    "Complete the relevant section in function **`TwoLayerNet.train`** to implement the scheduled learning rate. Implementation-wise you simply need to multiply `learning_rate` with  `learning_rate_decay`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) \n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "stats = net.train(X, y, X, y,\n",
    "            learning_rate=1e-1, \n",
    "            learning_rate_decay=0.95,\n",
    "            reg=5e-6,\n",
    "            num_epochs=40, verbose=0)\n",
    "\n",
    "print('Final training loss (no schedule): ', stats_no_schedule['loss_history'][-1])\n",
    "print('Final training loss (with schedule): ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats['loss_history'], 'b*-', label = 'with schedule')\n",
    "plt.plot(stats_no_schedule['loss_history'], 'ro-', label = 'without schedule')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, scheduled learning rate does not show any particular advantage because the problem is too simple (decaying the learning rate causes the parameter to be updated slower). However, in practice, a scheduled learning rate has been shown to be very useful and outperforms training without the scheme.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6: Show the training and validation result after each epoch**\n",
    "\n",
    "Finaly, show the training and validation result after each epoch.\n",
    "\n",
    "Complete the relevant section of function **`TwoLayerNet.train`** to compute the training and validation result after each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0) \n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "stats = net.train(X, y, X, y,\n",
    "            learning_rate=1e-1, \n",
    "            learning_rate_decay=0.95,\n",
    "            reg=5e-6,\n",
    "            num_epochs=40, verbose=2)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Assignment 3 (3 marks)\n",
    "\n",
    "Now that you have implemented a two-layer network that works on toy data, it's time to train our classifier on a real dataset. Apply the model you have built on CIFAR-10.\n",
    "\n",
    "In the following, we load the CIFAR-10 dataset, perform the training for 10 epochs and evaluate the performance on the training and evaluation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import cifar10\n",
    "from lib import common\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set the plot\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "# set to automatic reload\n",
    "%load_ext autoreload             \n",
    "%autoreload 2 \n",
    "\n",
    "# Load the training set\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = cifar10.get_CIFAR10_data (\n",
    "    r'.\\data\\cifar-10-batches-py', \n",
    "    num_training = 49000,\n",
    "    num_validation = 1000,\n",
    "    num_test = 10000,\n",
    "    num_dev = -1, \n",
    "    b_center = True,  \n",
    "    b_add_bias = False)\n",
    "\n",
    "classes = cifar10.get_classes()\n",
    "num_classes = len(classes)\n",
    "print('classes:', classes)\n",
    "print('Shape of training samples:', X_train.shape)\n",
    "print('Shape of validation samples:',  X_val.shape)\n",
    "print('Shape of testing samples:',  X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "\n",
    "# Construct the  network\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "print('\\n---- Training network -----')\n",
    "stats = net.train(\n",
    "            X_train, y_train, \n",
    "            X_val, y_val,\n",
    "            max_iters =1000, \n",
    "            batch_size=200,\n",
    "            learning_rate=1e-4, \n",
    "            learning_rate_decay=0.95,\n",
    "            reg=0.25, \n",
    "            verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (net.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug the training\n",
    "With the default parameters we provided above, you should get a validation accuracy of about 0.29 on the validation set. This is not very good.\n",
    "\n",
    "To find out what went wrong, we can do the following:\n",
    "* One strategy for getting insight into what's wrong is to *plot the loss function and the accuracies on the training and validation sets* during optimization.\n",
    "* Another strategy is to *visualize the weights in the first layer of the network*. In most neural networks trained on visual data, the first layer weights typically show some visible structure when visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], 'ro-', label='train')\n",
    "plt.plot(stats['val_acc_history'], 'b*--', label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's wrong?** Look at graphs plotted above. Check the model if it has the following issues: \n",
    "* If the loss is decreasing more or less linearly, this suggests that the learning rate may be too low. \n",
    "* If there is no gap between the training and validation accuracy, this suggests that the model we used has low capacity, and that we should increase its size. \n",
    "* On the other hand, with a very large model we would expect to see more overfitting, which would manifest itself as a very large gap between the training and validation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.common import visualize_grid\n",
    "\n",
    "# Visualize the weights of the network\n",
    "\n",
    "def show_net_weights(net):\n",
    "    W1 = net.params['W1']\n",
    "    W1 = W1.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)\n",
    "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_net_weights(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's wrong?** The code above visualize the weights in the first layer. This is done by reshaping individual row of the weight to fit the shape of the input image. In this sense, each row in the weight matrix represents a 'template image'. Notice that the template image are quite noisy and does not resemble any of the evaluated concept. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Tune your hyperparameters\n",
    "\n",
    "Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks. \n",
    "\n",
    "Below, you should experiment with different values of the various hyperparameters (*number of hidden units*, *learning rate*, *number of training epochs* and *regularization strength*). You might also consider tuning the *learning rate decay*, but you should be able to get good performance using the default value (0.95). Your goal in this exercise is to get as good of a result on CIFAR-10 as you can, with a fully-connected Neural Network. You should aim to achieve a classification accuracy of greater than 50% on both the validation set and testing set. \n",
    "\n",
    "Here are some guidelines:\n",
    "* To start, you may begin by tweaking the hyperparameter by hand to *find the working range* of each parameters. Start with the learning rate as it is the most important.                                                     \n",
    "* Then, write the code and use *Random Search* to sweep through possible combinations of hyperparameters automatically. You should do this in several runs, starting with a coarse range and slowly refining the range.          \n",
    "* To debug your network, you may want to use the *visualizations* similar to the ones we used above; The best setting will have significant qualitative differences from the ones we saw above for the poorly tuned network.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_net = None # store the best model into this \n",
    "\n",
    "#################################################################################\n",
    "# TODO:                                                                         # \n",
    "# Tune hyperparameters (learning rate, regularization strength and learning     #\n",
    "# rate decay using the validation set. Copy and modify the code from the        #\n",
    "# previous practical.                                                           # \n",
    "#                                                                               #\n",
    "# Save the trained model in 'best_net'                                          #\n",
    "#                                                                               #\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the weights of the best network\n",
    "show_net_weights(best_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on the test set\n",
    "When you are done experimenting, you should evaluate your final trained network on the test set; the test accuracy should be above 48%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = (best_net.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
