{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P02: Multi-class SVM Classifier on CIFAR10\n",
    "\n",
    "In this practical, you will practice putting together a simple classification pipeline. We shall work with the SVM linear classifier. The goals of this practical are as follows:\n",
    "\n",
    "1. Preprocess the data by **subtracting the mean image**\n",
    "2. Implement and apply a Multiclass Support Vector Machine (**SVM**) linear classifier\n",
    "3. Optimize the loss function with **SGD**\n",
    "4. Perform hyperparameter tuning of the learning rate and regularization strength using **random search**\n",
    "\n",
    "Once we have completed the task, we shall visualize the weights learnt by gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib import cifar10\n",
    "from lib import common\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the plot\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "# set to automatic reload\n",
    "%load_ext autoreload             \n",
    "%autoreload 2           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "### Loading and preparing the data\n",
    "\n",
    "First, we load and prepare our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = cifar10.load_data (r'.\\data\\cifar-10-batches-py')\n",
    "classes = cifar10.get_classes()\n",
    "num_classes = len(classes)\n",
    "print('classes:', classes)\n",
    "print('Shape of training samples:', X_train.shape)\n",
    "print('Shape of testing samples:',  X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the training, validation, testing and development set\n",
    "\n",
    "The following code split the data into *train*, *val*, and *test* sets. In addition we will create a small **development set** as a subset of the training data. We shall use this for code development (particularly to develop code to compute the loss function) so our code runs faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 500\n",
    "\n",
    "# Our validation set will be num_validation points from the original\n",
    "# training set.\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "# Our training set will be the first num_train points from the original\n",
    "# training set.\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# We will also make a development set, which is a small subset of\n",
    "# the training set.\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "# We use the first num_test points of the original test set as our\n",
    "# test set.\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 1: convert the dataset (X_train, X_val, X_test and X_dev) from uint8 to double**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.dtype      # uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.dtype      # expecting float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Center the data\n",
    "\n",
    "One of the common step is to **center the data**. This preprocessing step is important if the input is an image which has a range of [0, 255] where all values are positive. When all inputs are positive, this will slow down the training process. More details can be found in the lecture on \"Regular Neural Network\". \n",
    "\n",
    "<img src=\"imgs\\center.png\" width=\"60%\">\n",
    "\n",
    "\n",
    "**Exercise 2: Center all samples**\n",
    "\n",
    "First,compute the mean image by computing mean of all the images in the training samples. Then, display the mean image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtract the mean image from all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3: reshape the image data into rows**\n",
    "\n",
    "First, reshape the image data [?, 32, 32, 3] into rows [?, 3072] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# As a sanity check, print out the shapes of the data\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the bias term into W\n",
    "\n",
    "Lastly we append the bias dimension of ones (i.e. bias trick) so that our SVM only has to worry about optimizing a single weight matrix W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding the SVM Classifier\n",
    "\n",
    "\n",
    "** Exercise 4: Develop the function to compute the SVM loss function (non-vectorized version)** \n",
    "\n",
    "Now, let's develop the code to build our SVM classifier. Complete the function **`compute_loss_naive`** in **`lib/linear_svm.py`** which implements the multi-svm loss.\n",
    "\n",
    "<p><center>\n",
    "$L = \\frac{1}{N} \\sum_{i} L_i + \\lambda R(W)$\n",
    "</center></p>\n",
    "\n",
    "<p><center>\n",
    "$L_i = \\sum_{j\\neq y_i} max(0, s_j - s_{y_i} + 1) $\n",
    "</center></p>\n",
    "\n",
    "where $L$ is the multi-class SVM loss across the whole dataset, $L_i$ is the data loss for sample $i$ and $R(W)$ is the regularizer function, $y_i$ is the true label for sample $i$ and $s_j$ is the score of class $j$ for sample $i$.\n",
    "\n",
    "For regularization, we shall use the L2 norm.\n",
    "<p><center>\n",
    "$R(W) = \\sum_j\\sum_k W_{j,k}^2$\n",
    "</center></p>\n",
    "\n",
    "Complete the function **`compute_loss_naive`** which uses two `for` loops to evaluate the multiclass SVM loss function. Note that the **non-vectorized** implementation is not a very efficient one. We shall develop a more efficient vectorized version later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib.linear_svm import svm_loss_naive\n",
    "\n",
    "W = np.random.randn(3073, 10) * 0.0000000001 \n",
    "loss = svm_loss_naive(W, X_dev, y_dev, 0)\n",
    "\n",
    "print('loss: {:f}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have used a very small `W` for the code above.\n",
    "\n",
    "*Question: Since W is small and the regularization parameter has been turned off, what is the expected value for your SVM loss? \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_*\n",
    "\n",
    "Use your answer to check if your implementation above is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch Gradient Descent\n",
    "\n",
    "You have implemented the loss function. We are now ready to implement mini-batch  gradient descent to find the best parameter $W^*$ that minimizes the loss function $L$.\n",
    "\n",
    "\n",
    "**Exercise 5: Write the mini-batch gradient descent algorithm**\n",
    "\n",
    "Now, let's develop the code for the mini-batch gradient descent. Complete the function **`LinearClassifier.train`** in `lib/linear_classifier.py` which implements the gradient descent.\n",
    "\n",
    "If you completed the task successfully, you should see that the loss decreases from around 780 to 5.5 over the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib.linear_classifier import LinearSVM\n",
    "import time\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "svm = LinearSVM()\n",
    "loss_hist = svm.train(X_train, y_train, \n",
    "                      batch_size=200, \n",
    "                      learning_rate=1e-7, \n",
    "                      reg=2.5e4, \n",
    "                      num_epochs=5, \n",
    "                      vectorized = True, \n",
    "                      verbose=True)\n",
    "\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful debugging strategy is to plot the loss as a function of iteration number. The following code plots the graph loss vs iteration. The loss should decrease over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6: Write the prediction function**\n",
    "\n",
    "Now, let's develop the code to predict the labels for any samples. Complete the function **`LinearClassifier.train`** in `lib/linear_classifier.py` and then evaluate the performance on training and validation set.\n",
    "\n",
    "If you completed the task successfully, We are expecting an accuracy of about 3.6 or above for the model we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_pred = svm.predict(X_train)\n",
    "print('Training accuracy: {:f}'.format(np.mean(y_train == y_train_pred)))\n",
    "\n",
    "y_val_pred = svm.predict(X_val)\n",
    "print('validation accuracy: {:f}'.format(np.mean(y_val == y_val_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and hyperparameter tuning\n",
    "\n",
    "Now that we have developed all the necessary code, it's time to perform training. We shall use the validation set to finetune hyperparameters (regularization strength and learning rate). \n",
    "\n",
    "**Exercise 7: Write the cross-validation code (Random Search) for hyperparameter tuning**\n",
    "\n",
    "Write the code that chooses the best hyperparameters by tuning on the validation set. \n",
    "\n",
    "In the previous practical, we have used **Grid Search** for cross-validation. Today, we are going to use **Random Search** to determine our best hyperprameter on the validation set. Random search has been shown to produce comparable performance with lots of computational saving. Complete the code below to find the best hyperparameter settings.\n",
    "\n",
    "*Hint*: You should use a small value for the number of batch iterations (`max_iter = 100`) as you develop your validation code so that the SVMs don't take much time to train. Once you are confident that your validation code works, you should rerun the validation code with a larger number of iterations (`max_iter = -1`).                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = {}               # results is dictionary mapping (learning_rate, regularization_strength) \n",
    "                           # to (training_accuracy, validation_accuracy). The accuracy is simply the \n",
    "                           #fraction of data points that are correctly classified.\n",
    "best_val = -1              # The highest validation accuracy that we have seen so far.\n",
    "best_svm = None            # The LinearSVM object that achieved the highest validation rate.\n",
    "\n",
    "# hyperparameter settings\n",
    "learning_rates = [-7, -2]            \n",
    "regularization_strengths = [-4, 3]\n",
    "num_trials = 10\n",
    "\n",
    "for i in range(num_trials): \n",
    "    \n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # sample the learning rate and regularization hyperparameters in log space     #\n",
    "    ################################################################################\n",
    "    # lr  = ...\n",
    "    # reg = ...\n",
    "    ################################################################################\n",
    "    #                       END OF YOUR CODE                                       #\n",
    "    ################################################################################\n",
    "    \n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Train on the training set. Set number of epochs to 7, maximum number of      #\n",
    "    #     iteration to 100, verbose to False. Set to use the vectorized version if #\n",
    "    #     possible.                                                                # \n",
    "    ################################################################################\n",
    "    # Your code here \n",
    "    ################################################################################\n",
    "    #                       END OF YOUR CODE                                       #\n",
    "    ################################################################################\n",
    "        \n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Evaluate your model on both the training and validation set                  #\n",
    "    ################################################################################\n",
    "    # your code here\n",
    "    ################################################################################\n",
    "    #                       END OF YOUR CODE                                       #\n",
    "    ################################################################################\n",
    "        \n",
    "    results[(lr, reg)] = (train_accuracy, val_accuracy)\n",
    "        \n",
    "    if val_accuracy > best_val:\n",
    "        best_val = val_accuracy\n",
    "        best_svm = svm\n",
    "        \n",
    "    print('iter {:d}: lr {:e} reg {:e} train accuracy: {:.4f} val accuracy: {:.4f}'.format(\n",
    "                i, lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "\n",
    "# Sort the validation result for easy viewing\n",
    "import operator\n",
    "sorted_results = sorted(results.items(), key=operator.itemgetter(1))\n",
    "print('\\n---------------------------------- Sorted result ----------------------------------')\n",
    "for ((lr, reg), (train_acc, val_acc)) in sorted_results:\n",
    "    print('lr {:e}, reg{:e}: train accuracy: {:.4f} val accuracy: {:.4f}'.format(lr, reg, train_acc, val_acc))\n",
    "print('>> Best validation: {:.4f}'.format(best_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8: Optimize your hyperparameters on the validation set** \n",
    "\n",
    "Hyperparameter tuning is performed in stages. The first stage is to find the range of working range for hyperparameters (we have done this for you). Then, yYou should repeat the above process using a finer ranges for the learning rate and regularization strength. You may repeat this several times until you are satisfied with the performance of your system on the validation set. If you are careful you should be able to get a classification accuracy of about 0.4 on the validation set. \n",
    "\n",
    "*What is the best accuracy that you can achieve? What is the finest range of learning rates and regularization strength that you evaluate on?*\n",
    "<br> **Answer**: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Analysis\n",
    "\n",
    "### Evaluating on the testing set\n",
    "\n",
    "**Exercise 9: Evaluate on the test set**\n",
    "\n",
    "From above, we determine the best hyperparameter settings through cross-validation. The best model has also been saved as `best_svm`. The following code shows the prediction resut on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the template\n",
    "\n",
    "Now, let's visualize the learnt weights for each class. Depending on your choice of learning rate and regularization strength, these may or may not be nice to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = best_svm.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "      \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Course Assignment 2 [2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Submission**:\n",
    "\n",
    "This is the second coursework assignment which extends this practical. Complete this section and upload the following file to WBLE by 11 Feb 2018. \n",
    "* `linear_svm.py`\n",
    "\n",
    "\n",
    "**Task: Vectorized version of the multi-class SVM loss**.\n",
    "\n",
    "For this assignment, your task is to implement the vectorized version of the multiclass svm loss function. Complete the function **`svm_loss_vectorized`** in `linear_svm.py`. The function should NOT contain any for loops. \n",
    "\n",
    "After you have completed your assignment, verify your implementation by running the code below. The loss computed by the vectorized version must be equal to the naive version. The vectorized version should be much more efficient than the non-vectorized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib.linear_svm import svm_loss_naive\n",
    "from lib.linear_svm import svm_loss_vectorized\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "W = np.random.randn(3073, 10) * 0.0000000001 \n",
    "loss_naive = svm_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('Naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "loss_vectorized = svm_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('Vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# The losses should match but your vectorized implementation should be much faster.\n",
    "print('difference: {:.2f}'.format(loss_naive - loss_vectorized))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfgpu]",
   "language": "python",
   "name": "conda-env-tfgpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
